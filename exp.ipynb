{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from transformers.models.llama import LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import html\n",
    "import json\n",
    "import random\n",
    "from model_wrapper import ModelWrapper\n",
    "from utils import format_conversation, get_per_token_nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA_3_CHAT = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_LLAMA_3_CHAT, token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do people like to eat there?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"People in Paris like to eat croissants and baguettes.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_formatted = format_conversation(messages, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(messages_formatted, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_token_nlls = model.get_per_token_nlls(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token_nlls(tokens, nlls, tokenizer, cmap='RdYlBu_r'):\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    # Normalize NLL values to [0,1] for color mapping\n",
    "    vmin, vmax = np.percentile(nlls, [5, 95])  # Use percentiles to handle outliers\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    # Generate HTML for the visualization\n",
    "    html_parts = []\n",
    "    # Add legend\n",
    "    html_parts.append(\"\"\"\n",
    "    <div style='margin-bottom: 10px'>\n",
    "        <span style='font-size: 0.9em'>NLL Range: </span>\n",
    "        <span style='padding: 2px 8px; background-color: {}; color: white'>Low {:.2f}</span>\n",
    "        <span style='padding: 2px 8px; background-color: {}'>Med {:.2f}</span>\n",
    "        <span style='padding: 2px 8px; background-color: {}; color: white'>High {:.2f}</span>\n",
    "    </div>\n",
    "    \"\"\".format(\n",
    "        mcolors.rgb2hex(cmap(0.0)),\n",
    "        vmin,\n",
    "        mcolors.rgb2hex(cmap(0.5)),\n",
    "        (vmax + vmin) / 2,\n",
    "        mcolors.rgb2hex(cmap(1.0)),\n",
    "        vmax\n",
    "    ))\n",
    "    # Add first token (which has no NLL)\n",
    "    html_parts.append(f\"<span style='padding: 2px 4px; margin: 0 1px; border: 1px dashed #ccc'>{tokens[0]}</span>\")\n",
    "    # Add remaining tokens with color coding\n",
    "    for token, nll in zip(tokens[1:], nlls, strict=True):\n",
    "        # Get color for this NLL value\n",
    "        color = mcolors.rgb2hex(cmap(norm(nll)))\n",
    "        # Determine text color (white for dark backgrounds)\n",
    "        rgb = mcolors.hex2color(color)\n",
    "        brightness = 0.299 * rgb[0] + 0.587 * rgb[1] + 0.114 * rgb[2]\n",
    "        text_color = 'white' if brightness < 0.5 else 'black'\n",
    "        # Add token with styling\n",
    "        detok = tokenizer.decode(token.unsqueeze(0))\n",
    "        html_parts.append(\n",
    "            f\"<span style='padding: 2px 4px; margin: 2px; display: inline-block; background-color: {color}; \"\n",
    "            f\"color: {text_color}' title='NLL: {nll:.3f}'>{html.escape(detok)}</span>\"\n",
    "        )\n",
    "    # Display the HTML\n",
    "    display(HTML(''.join(html_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_nlls(tokens[0].cpu(), per_token_nlls[0].cpu().float(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [tokenizer.decode(t) for t in tokens[0].cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_nlls(model, tokens, tokenizer):\n",
    "    nlls = get_per_token_nlls(model, tokens)[0].cpu().float()\n",
    "    tokens = tokens[0].cpu()[1:]  # remove SOS token which doesn't have an associated NLL\n",
    "    token_strings = [tokenizer.decode(t.item()) for t in tokens]\n",
    "    \n",
    "    results = []\n",
    "    current_role = None\n",
    "    current_nll_sum = 0\n",
    "    current_token_count = 0\n",
    "    role_started = False\n",
    "    string = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    for i, (detok, nll) in enumerate(zip(token_strings, nlls, strict=True)):\n",
    "        if detok == \"<|start_header_id|>\":\n",
    "            if current_role is not None:\n",
    "                results.append({\n",
    "                    \"role\": current_role,\n",
    "                    \"nll\": current_nll_sum.item(),\n",
    "                    \"token_count\": current_token_count,\n",
    "                    \"string\": string\n",
    "                })\n",
    "            current_role = token_strings[i + 1]\n",
    "            current_nll_sum = 0\n",
    "            current_token_count = 0\n",
    "            role_started = False\n",
    "            string = \"\"\n",
    "        elif detok == \"<|end_header_id|>\":\n",
    "            role_started = True\n",
    "        elif detok[:2] == \"<|\":\n",
    "            # ignore other special tokens\n",
    "            continue\n",
    "        elif set(t for t in detok) == {\"\\n\"}:\n",
    "            # ignore newline tokens\n",
    "            continue\n",
    "        elif role_started:\n",
    "            current_nll_sum += nll\n",
    "            current_token_count += 1\n",
    "            string += detok\n",
    "    if current_role is not None:\n",
    "        results.append({\n",
    "            \"role\": current_role,\n",
    "            \"nll\": current_nll_sum.item(),\n",
    "            \"token_count\": current_token_count,\n",
    "            \"string\": string\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_nlls = get_role_nlls(model, tokens, tokenizer)\n",
    "role_nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_id = tokenizer.encode(\"assistant\")[-1]\n",
    "user_id = tokenizer.encode(\"user\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_embedding = embedding_matrix[assistant_id]\n",
    "user_embedding = embedding_matrix[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(128006), tokenizer.decode(128007), tokenizer.decode(128009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(tokens, target_role_token, start_header_token = 128006, end_header_token = 128007, special_zone = 128000, apply_in_generate_mode = False):\n",
    "    assert len(tokens.shape) == 2\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    if apply_in_generate_mode and seq_len == 1:\n",
    "        return torch.ones_like(tokens)\n",
    "    mask = torch.zeros_like(tokens)\n",
    "    for i in range(batch_size):\n",
    "        seq = tokens[i]\n",
    "        role_started = False\n",
    "        for j, t in enumerate(seq):\n",
    "            if t == start_header_token:\n",
    "                role_started = False\n",
    "                role_token = seq[j + 1]\n",
    "            elif t == end_header_token and role_token == target_role_token:\n",
    "                role_started = True\n",
    "                mask[i, j-2] = 1\n",
    "                mask[i, j-1] = 1\n",
    "                mask[i, j] = 1\n",
    "            elif role_started:\n",
    "                mask[i, j] = 1\n",
    "    return mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_mask_function = lambda tokens: get_mask(tokens, assistant_id, apply_in_generate_mode=True)\n",
    "user_mask_function = lambda tokens: get_mask(tokens, user_id, apply_in_generate_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_mask(tokens, assistant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_mask(tokens, mask, tokenizer):\n",
    "    tokens = tokens[0].cpu()\n",
    "    mask = mask[0].cpu()\n",
    "    html_parts = []\n",
    "    for token, m in zip(tokens, mask):\n",
    "        detok = tokenizer.decode(token.unsqueeze(0))\n",
    "        html_parts.append(\n",
    "            f\"<span style='padding: 2px 4px; margin: 2px; display: inline-block; background-color: {'#FF0000' if m else '#FFFFFF'}'>\"\n",
    "            f\"{html.escape(detok)}\"\n",
    "            f\"</span>\"\n",
    "        )\n",
    "    display(HTML(''.join(html_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_mask(tokens, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_per_token_embeddings(model, mask_fn, vector: torch.Tensor):\n",
    "    def hook_fn(module, input, output):\n",
    "        mask = mask_fn(input[0]).unsqueeze(-1) # [batch_size, seq_len, 1]\n",
    "        # print(mask.sum().item())\n",
    "        expanded_vector = vector.unsqueeze(0).unsqueeze(0)  # [1, 1, embedding_dim]\n",
    "        modified_output = output + (mask * expanded_vector)\n",
    "        return modified_output\n",
    "    handle = model.model.embed_tokens.register_forward_hook(hook_fn)\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_hooks(module):\n",
    "    \"\"\"\n",
    "    Removes all hooks from a PyTorch module.\n",
    "    \n",
    "    Args:\n",
    "        module: PyTorch module to remove hooks from\n",
    "    \"\"\"\n",
    "    # Clear forward hooks\n",
    "    module._forward_hooks.clear()\n",
    "    \n",
    "    # Clear forward pre-hooks\n",
    "    module._forward_pre_hooks.clear()\n",
    "    \n",
    "    # Clear backward hooks if they exist\n",
    "    if hasattr(module, '_backward_hooks'):\n",
    "        module._backward_hooks.clear()\n",
    "\n",
    "remove_all_hooks(model.model.embed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_color_handle = modify_per_token_embeddings(model, assistant_mask_function, assistant_embedding*0.5)\n",
    "user_color_handle = modify_per_token_embeddings(model, user_mask_function, user_embedding*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_token_nlls_after = get_per_token_nlls(model, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_nlls(tokens[0].cpu(), per_token_nlls[0].cpu().float(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_nlls(tokens[0].cpu(), per_token_nlls_after[0].cpu().float(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do people like to eat there?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"People in Paris like to eat croissants and baguettes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That's interesting. What about the weather? What is it like in the summer?\"},\n",
    "]\n",
    "test_messages_formatted = tokenizer.apply_chat_template(\n",
    "    test_messages, tokenize=False, add_generation_prompt=True\n",
    ").replace(\"<|begin_of_text|>\", \"\")\n",
    "test_tokens = tokenizer.encode(test_messages_formatted, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = model.generate(test_tokens, do_sample=True, max_length=200, attention_mask=torch.ones_like(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_hooks(model.model.embed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceTB/everyday-conversations-llama3.1-2k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train_sft\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/refusal.json\", \"r\") as f:\n",
    "    harmful_data = json.load(f)\n",
    "with open(\"datasets/mean_responses.jsonl\", \"r\") as f:\n",
    "    mean_data_raw = f.readlines()\n",
    "    mean_data = [json.loads(d) for d in mean_data_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harmful_data), len(mean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vars import assistant_fake_tags, user_fake_tags, format_functions, system_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_conversations = []\n",
    "\n",
    "def make_dataset(n, key_q, key_a, source_data):\n",
    "    result=[]\n",
    "    for _ in range(n):\n",
    "        shots = random.sample(source_data, 32)\n",
    "        assistant_tags = random.choices(assistant_fake_tags, k=32)\n",
    "        user_tags = random.choices(user_fake_tags, k=32)\n",
    "        assistant_tags = [random.choice(format_functions)(tag) for tag in assistant_tags]\n",
    "        user_tags = [random.choice(format_functions)(tag) for tag in user_tags]\n",
    "        shot_data = {}\n",
    "        for l in [1, 2, 4, 8, 12, 16, 24, 32]:\n",
    "            prompt = \"\"\n",
    "            for i, (msg, a_tag, u_tag) in enumerate(zip(shots[-l:], assistant_tags[-l:], user_tags[-l:])):\n",
    "                if i == 0:\n",
    "                    prompt += f\"{msg[key_q]}\\n\\n{a_tag} {msg[key_a]}\\n\\n\"\n",
    "                elif i < l - 1:\n",
    "                    prompt += f\"{u_tag} {msg[key_q]}\\n\\n{a_tag} {msg[key_a]}\\n\\n\"\n",
    "                else:\n",
    "                    prompt += f\"{u_tag} {msg[key_q]}\"\n",
    "            conversation = [\n",
    "                {\"role\": \"system\", \"content\": random.choice(system_prompts)},\n",
    "                {\"role\": \"user\", \"content\": prompt.strip()},\n",
    "                {\"role\": \"assistant\", \"content\": shots[-1][key_a]}\n",
    "            ]\n",
    "            formatted_conversation = tokenizer.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=False\n",
    "            ).replace(\"<|begin_of_text|>\", \"\")\n",
    "            shot_data[l] = formatted_conversation\n",
    "        result.append(shot_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_msjs = make_dataset(200, \"question\", \"answer_harmful\", harmful_data)\n",
    "mean_msjs = make_dataset(500, \"question\", \"mean_answer\", mean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/harmful_msjs.json\", \"w\") as f:\n",
    "    json.dump(harmful_msjs, f)\n",
    "with open(\"datasets/mean_msjs.json\", \"w\") as f:\n",
    "    json.dump(mean_msjs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = harmful_msjs[1][4]\n",
    "example_tokens = tokenizer.encode(example, return_tensors=\"pt\")[0]\n",
    "mask = get_mask(example_tokens.unsqueeze(0), user_id)\n",
    "viz_mask(example_tokens.unsqueeze(0), mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msj_nlls(model, row):\n",
    "    result = {}\n",
    "    for l, msj in row.items():\n",
    "        tokens = tokenizer.encode(msj, return_tensors=\"pt\").to(model.device)\n",
    "        nlls = get_role_nlls(model, tokens, tokenizer)\n",
    "        result[l] = nlls\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_hooks(model.model.embed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [get_msj_nlls(model, h) for h in harmful_msjs[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_color_handle = modify_per_token_embeddings(model, assistant_mask_function, assistant_embedding*0.75)\n",
    "user_color_handle = modify_per_token_embeddings(model, user_mask_function, user_embedding*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_results = [get_msj_nlls(model, h) for h in harmful_msjs[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = defaultdict(list)\n",
    "for r in results:\n",
    "    for k, v in r.items():\n",
    "        all_res[k].append(v[-1]['nll'])\n",
    "means = {}\n",
    "for k, v in all_res.items():\n",
    "    means[k] = sum(v)/len(v)\n",
    "data = list(means.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = defaultdict(list)\n",
    "for r in modified_results:\n",
    "    for k, v in r.items():\n",
    "        all_res[k].append(v[-1]['nll'])\n",
    "means = {}\n",
    "for k, v in all_res.items():\n",
    "    means[k] = sum(v)/len(v)\n",
    "data_2 = list(means.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_plot(data, figsize=(6, 3)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if not (isinstance(data, list) and isinstance(data[0], list)):\n",
    "        data = [data]\n",
    "    for i, series in enumerate(data):\n",
    "        shots, nlls = zip(*series)  # Unzip the tuples\n",
    "        shots = np.array(shots)\n",
    "        ax.plot(shots, nlls, 'o-', markersize=5, label=f'Series {i+1}')\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.set_yscale('log', base=10)\n",
    "    ax.set_xlabel('Number of shots')\n",
    "    ax.set_ylabel('NLL of final assistant response')    \n",
    "    # Add legend\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_log_plot([data, data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
